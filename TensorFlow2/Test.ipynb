{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "X = tf.constant([[1., 2.], [3., 4.]])\n",
    "y = tf.constant([[1.], [2.]])\n",
    "w = tf.Variable(initial_value=[[1.], [2.]])\n",
    "b = tf.Variable(initial_value=1.)\n",
    "with tf.GradientTape() as tape:\n",
    "    L = tf.reduce_sum(tf.square(tf.matmul(X, w) + b - y))\n",
    "w_grad, b_grad = tape.gradient(L, [w, b])        # 计算L(w, b)关于w, b的偏导数\n",
    "print(L, w_grad, b_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#线性回归，numpy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X_raw = np.array([2013, 2014, 2015, 2016, 2017], dtype = np.float32)\n",
    "y_raw = np.array([12000, 14000, 15000, 16500, 17500], dtype = np.float32)\n",
    "\n",
    "X = (X_raw - X_raw.min()) / (X_raw.max() - X_raw.min())\n",
    "y = (y_raw - y_raw.min()) / (y_raw.max() - y_raw.min())\n",
    "\n",
    "a, b = 0, 0\n",
    "\n",
    "num_epoch = 10000\n",
    "learning_rate = 5e-4\n",
    "for e in range(num_epoch):\n",
    "    #手动计算梯度\n",
    "    y_pred = a * X + b\n",
    "    grad_a, grad_b = 2 * (y_pred - y).dot(X), 2 * (y_pred - y).sum()\n",
    "    \n",
    "    #更新参数\n",
    "    a, b = a - learning_rate * grad_a, b - learning_rate * grad_b\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#tf构建线性回归\n",
    "X = tf.constant(X)\n",
    "y = tf.constant(y)\n",
    "\n",
    "a = tf.Variable(initial_value=0.)\n",
    "b = tf.Variable(initial_value=0.)\n",
    "variables = [a, b]\n",
    "\n",
    "num_epoch = 10000\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=5e-4)\n",
    "for e in range(num_epoch):\n",
    "    #使用tf.GradientTape()记录损失函数的梯度信息\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = a * X + b\n",
    "        loss = tf.reduce_sum(tf.square(y_pred-y))\n",
    "    #tf自动计算损失函数关于自变量的梯度\n",
    "    grads = tape.gradient(loss, variables)\n",
    "    #tf自动根据梯度更新参数\n",
    "    optimizer.apply_gradients(grads_and_vars = zip(grads, variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型类编写线性模型\n",
    "import tensorflow as tf\n",
    "X = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "y = tf.constant([[10.0], [20.0]])\n",
    "\n",
    "class Linear(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            units = 1,\n",
    "            activation = None,\n",
    "            kernel_initializer = tf.zeros_initializer(),\n",
    "            bias_initializer = tf.zeros_initializer()\n",
    "        )\n",
    "        \n",
    "    def call(self, input):\n",
    "        output = self.dense(input)\n",
    "        return output\n",
    "    \n",
    "#以下代码类似前节\n",
    "model = Linear()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate = 0.01)\n",
    "for i in  range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.reduce_mean(tf.square(y_pred - y))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars = zip(grads, model.variables))\n",
    "print(model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------\n",
    "#多层感知机\n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(units = 100, activation = tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units = 10)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.flatten(inputs)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#卷积神经网络\n",
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters = 32,                 #卷积层神经元数据\n",
    "            kernel_size = [5, 5],         #感受野大小\n",
    "            padding = 'same',             #padding策略\n",
    "            activation = tf.nn.relu       #激活函数\n",
    "        )\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size = [2, 2], strides = 2)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters = 64,\n",
    "            kernel_size = [5, 5],\n",
    "            padding = 'same',\n",
    "            activation = tf.nn.relu\n",
    "        )\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size = [2, 2], strides = 2)\n",
    "        self.flatten = tf.keras.layers.Reshape(target_shape = (7 * 7 * 64,))\n",
    "        self.dense1 = tf.keras.layers.Dense(units = 1024, activation = tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units = 10)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)                  # [batch_size, 28, 28, 32]\n",
    "        x = self.pool1(x)                       # [batch_size, 14, 14, 32]\n",
    "        x = self.conv2(x)                       # [batch_size, 14, 14, 64]\n",
    "        x = self.pool2(x)                       # [batch_size, 7, 7, 64]\n",
    "        x = self.flatten(x)                     # [batch_size, 7 * 7 * 64]\n",
    "        x = self.dense1(x)                      # [batch_size, 1024]\n",
    "        x = self.dense2(x)                      # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # MNIST中的图像默认为uint8（0-255的数字）。以下代码将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        # 从数据集中随机取出batch_size个元素并返回\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]\n",
    "\n",
    "#-------------------------------------------------------\n",
    "#模型的训练\n",
    "num_epochs = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "#此处切换模型\n",
    "model = CNN()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "\n",
    "#-------------------------------------------------------\n",
    "#从 DataLoader 中随机取一批训练数据；\n",
    "#将这批数据送入模型，计算出模型的预测值；\n",
    "#将模型预测值与真实值进行比较，计算损失函数（loss）。这里使用 tf.keras.losses 中的交叉熵函数作为损失函数；\n",
    "#计算损失函数关于模型变量的导数；\n",
    "#将求出的导数值传入优化器，使用优化器的 apply_gradients 方法更新模型参数以最小化损失函数（优化器的详细使用方法见 前章 ）。\n",
    "#-------------------------------------------------------\n",
    "\n",
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true = y, y_pred = y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        if batch_index % 100 == 0:\n",
    "            print(\"batch {:d} : loss {:f}\".format(batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars = zip(grads, model.variables))\n",
    "#-------------------------------------------------------\n",
    "\n",
    "#-------------------------------------------------------\n",
    "#模型的评估\n",
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index : end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true = data_loader.test_label[start_index : end_index], y_pred = y_pred)\n",
    "    print(\"test accuracy: {:f}\".format(sparse_categorical_accuracy.result()))\n",
    "\n",
    "\n",
    "#-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <加载预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "num_epoch = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "dataset = tfds.load(\"tf_flowers\", split = tfds.Split.TRAIN, as_supervised = True)\n",
    "dataset = dataset.map(lambda img, label : \n",
    "                      (tf.image.resize(img, (224, 224)) / 255.0, label)).shuffle(1024).batch(batch_size)\n",
    "model = tf.keras.applications.MobileNetV2(weights = None, classes = 5)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "for e in range(num_epoch):\n",
    "    for images, labels in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            labels_pred = model(images, training = True)\n",
    "            loss = tf.keras.losses.sparse_categorical_crossentropy(y_true = labels, y_pred = labels_pred)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            print(\"loss {:f}\".format(loss.numpy()))\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(grads_and_vars = zip(grads, model.trainable_variables))\n",
    "    print(labels_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载预训练模型>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self):\n",
    "        path = tf.keras.utils.get_file('nietzsche.txt',\n",
    "            origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "        with open(path, encoding = 'utf-8') as f:\n",
    "            self.raw_text = f.read().lower()\n",
    "        self.chars = sorted(list(set(self.raw_text)))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        self.text = [self.char_indices[c] for c in self.raw_text]\n",
    "        \n",
    "    def get_batch(self, seq_length, batch_size):\n",
    "        seq = []\n",
    "        next_char = []\n",
    "        for i in range(batch_size):\n",
    "            index = np.random.randint(0, len(self.text) - seq_length)\n",
    "            seq.append(self.text[index : index + seq_length])\n",
    "            next_char.append(self.text[index + seq_length])\n",
    "        return np.array(seq), np.array(next_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self, num_chars, batch_size, seq_length):\n",
    "        super().__init__()\n",
    "        self.num_chars = num_chars\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.cell = tf.keras.layers.LSTMCell(units = 256)\n",
    "        self.dense = tf.keras.layers.Dense(units = self.num_chars)\n",
    "    \n",
    "    def call(self, inputs, from_logits=False):\n",
    "        inputs = tf.one_hot(inputs, depth=self.num_chars)       # [batch_size, seq_length, num_chars]\n",
    "        state = self.cell.get_initial_state(batch_size=self.batch_size, dtype=tf.float32)   # 获得 RNN 的初始状态\n",
    "        for t in range(self.seq_length):\n",
    "            output, state = self.cell(inputs[:, t, :], state)   # 通过当前输入和前一时刻的状态，得到输出和当前时刻的状态\n",
    "        logits = self.dense(output)\n",
    "        if from_logits:                                         # from_logits 参数控制输出是否通过 softmax 函数进行归一化\n",
    "            return logits\n",
    "        else:\n",
    "            return tf.nn.softmax(logits)\n",
    "        \n",
    "    def predict(self, inputs, temperature = 1.):\n",
    "        batch_size, _ = tf.shape(inputs)\n",
    "        logits = self(inputs, from_logits=True)                         # 调用训练好的RNN模型，预测下一个字符的概率分布\n",
    "        prob = tf.nn.softmax(logits / temperature).numpy()              # 使用带 temperature 参数的 softmax 函数获得归一化的概率分布值\n",
    "        return np.array([np.random.choice(self.num_chars, p=prob[i, :]) # 使用 np.random.choice 函数，\n",
    "                         for i in range(batch_size.numpy())])           # 在预测的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_batches = 1000\n",
    "seq_length = 40\n",
    "batch_size = 50\n",
    "learning_rate = 1e-3\n",
    "\n",
    "data_loader = DataLoader()\n",
    "model = RNN(num_chars = len(data_loader.chars), batch_size = batch_size, seq_length = seq_length)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(seq_length, batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true = y, y_pred = y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch {:d}: loss {:f}\".format(batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars = zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_, _ = data_loader.get_batch(seq_length, 1)\n",
    "for diversity in [0.2, 0.5, 1.0, 1.2]:      # 丰富度（即temperature）分别设置为从小到大的 4 个值\n",
    "    X = X_\n",
    "    print(\"diversity %f:\" % diversity)\n",
    "    for t in range(400):\n",
    "        y_pred = model.predict(X, diversity)    # 预测下一个字符的编号\n",
    "        print(data_loader.indices_char[y_pred[0]], end='', flush=True)  # 输出预测的字符\n",
    "        X = np.concatenate([X[:, 1:], np.expand_dims(y_pred, axis=1)], axis=-1)     # 将预测的字符接在输入 X 的末尾，并截断 X 的第一个字符，以保证 X 的长度不变\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## RNN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## <Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpyt as np\n",
    "import gym\n",
    "import random\n",
    "from collections import deque\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}