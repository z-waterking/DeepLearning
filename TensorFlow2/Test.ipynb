{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "X = tf.constant([[1., 2.], [3., 4.]])\n",
    "y = tf.constant([[1.], [2.]])\n",
    "w = tf.Variable(initial_value=[[1.], [2.]])\n",
    "b = tf.Variable(initial_value=1.)\n",
    "with tf.GradientTape() as tape:\n",
    "    L = tf.reduce_sum(tf.square(tf.matmul(X, w) + b - y))\n",
    "w_grad, b_grad = tape.gradient(L, [w, b])        # 计算L(w, b)关于w, b的偏导数\n",
    "print(L, w_grad, b_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#线性回归，numpy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X_raw = np.array([2013, 2014, 2015, 2016, 2017], dtype = np.float32)\n",
    "y_raw = np.array([12000, 14000, 15000, 16500, 17500], dtype = np.float32)\n",
    "\n",
    "X = (X_raw - X_raw.min()) / (X_raw.max() - X_raw.min())\n",
    "y = (y_raw - y_raw.min()) / (y_raw.max() - y_raw.min())\n",
    "\n",
    "a, b = 0, 0\n",
    "\n",
    "num_epoch = 10000\n",
    "learning_rate = 5e-4\n",
    "for e in range(num_epoch):\n",
    "    #手动计算梯度\n",
    "    y_pred = a * X + b\n",
    "    grad_a, grad_b = 2 * (y_pred - y).dot(X), 2 * (y_pred - y).sum()\n",
    "    \n",
    "    #更新参数\n",
    "    a, b = a - learning_rate * grad_a, b - learning_rate * grad_b\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#tf构建线性回归\n",
    "X = tf.constant(X)\n",
    "y = tf.constant(y)\n",
    "\n",
    "a = tf.Variable(initial_value=0.)\n",
    "b = tf.Variable(initial_value=0.)\n",
    "variables = [a, b]\n",
    "\n",
    "num_epoch = 10000\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=5e-4)\n",
    "for e in range(num_epoch):\n",
    "    #使用tf.GradientTape()记录损失函数的梯度信息\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = a * X + b\n",
    "        loss = tf.reduce_sum(tf.square(y_pred-y))\n",
    "    #tf自动计算损失函数关于自变量的梯度\n",
    "    grads = tape.gradient(loss, variables)\n",
    "    #tf自动根据梯度更新参数\n",
    "    optimizer.apply_gradients(grads_and_vars = zip(grads, variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型类编写线性模型\n",
    "import tensorflow as tf\n",
    "X = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "y = tf.constant([[10.0], [20.0]])\n",
    "\n",
    "class Linear(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            units = 1,\n",
    "            activation = None,\n",
    "            kernel_initializer = tf.zeros_initializer(),\n",
    "            bias_initializer = tf.zeros_initializer()\n",
    "        )\n",
    "        \n",
    "    def call(self, input):\n",
    "        output = self.dense(input)\n",
    "        return output\n",
    "    \n",
    "#以下代码类似前节\n",
    "model = Linear()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate = 0.01)\n",
    "for i in  range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.reduce_mean(tf.square(y_pred - y))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars = zip(grads, model.variables))\n",
    "print(model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------\n",
    "#多层感知机\n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(units = 100, activation = tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units = 10)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.flatten(inputs)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#卷积神经网络\n",
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters = 32,                 #卷积层神经元数据\n",
    "            kernel_size = [5, 5],         #感受野大小\n",
    "            padding = 'same',             #padding策略\n",
    "            activation = tf.nn.relu       #激活函数\n",
    "        )\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size = [2, 2], strides = 2)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters = 64,\n",
    "            kernel_size = [5, 5],\n",
    "            padding = 'same',\n",
    "            activation = tf.nn.relu\n",
    "        )\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size = [2, 2], strides = 2)\n",
    "        self.flatten = tf.keras.layers.Reshape(target_shape = (7 * 7 * 64,))\n",
    "        self.dense1 = tf.keras.layers.Dense(units = 1024, activation = tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units = 10)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)                  # [batch_size, 28, 28, 32]\n",
    "        x = self.pool1(x)                       # [batch_size, 14, 14, 32]\n",
    "        x = self.conv2(x)                       # [batch_size, 14, 14, 64]\n",
    "        x = self.pool2(x)                       # [batch_size, 7, 7, 64]\n",
    "        x = self.flatten(x)                     # [batch_size, 7 * 7 * 64]\n",
    "        x = self.dense1(x)                      # [batch_size, 1024]\n",
    "        x = self.dense2(x)                      # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 : loss 2.305252\n",
      "batch 100 : loss 0.188559\n",
      "batch 200 : loss 0.117815\n",
      "batch 300 : loss 0.100690\n",
      "batch 400 : loss 0.005754\n",
      "batch 500 : loss 0.008916\n",
      "batch 600 : loss 0.156306\n",
      "batch 700 : loss 0.045422\n",
      "batch 800 : loss 0.088421\n",
      "batch 900 : loss 0.006576\n",
      "batch 1000 : loss 0.112557\n",
      "batch 1100 : loss 0.002607\n",
      "batch 1200 : loss 0.018561\n",
      "batch 1300 : loss 0.017290\n",
      "batch 1400 : loss 0.004588\n",
      "batch 1500 : loss 0.006743\n",
      "batch 1600 : loss 0.052868\n",
      "batch 1700 : loss 0.001867\n",
      "batch 1800 : loss 0.035089\n",
      "batch 1900 : loss 0.000640\n",
      "batch 2000 : loss 0.009406\n",
      "batch 2100 : loss 0.002940\n",
      "batch 2200 : loss 0.000591\n",
      "batch 2300 : loss 0.004280\n",
      "batch 2400 : loss 0.013685\n",
      "batch 2500 : loss 0.003754\n",
      "batch 2600 : loss 0.001094\n",
      "batch 2700 : loss 0.004257\n",
      "batch 2800 : loss 0.000329\n",
      "batch 2900 : loss 0.007263\n",
      "batch 3000 : loss 0.064822\n",
      "batch 3100 : loss 0.003088\n",
      "batch 3200 : loss 0.002815\n",
      "batch 3300 : loss 0.001333\n",
      "batch 3400 : loss 0.011974\n",
      "batch 3500 : loss 0.000504\n",
      "batch 3600 : loss 0.001722\n",
      "batch 3700 : loss 0.000994\n",
      "batch 3800 : loss 0.003573\n",
      "batch 3900 : loss 0.006617\n",
      "batch 4000 : loss 0.124040\n",
      "batch 4100 : loss 0.021516\n",
      "batch 4200 : loss 0.039780\n",
      "batch 4300 : loss 0.002223\n",
      "batch 4400 : loss 0.001150\n",
      "batch 4500 : loss 0.000200\n",
      "batch 4600 : loss 0.000417\n",
      "batch 4700 : loss 0.001714\n",
      "batch 4800 : loss 0.004837\n",
      "batch 4900 : loss 0.045657\n",
      "batch 5000 : loss 0.000298\n",
      "batch 5100 : loss 0.000250\n",
      "batch 5200 : loss 0.000215\n",
      "batch 5300 : loss 0.028085\n",
      "batch 5400 : loss 0.000147\n",
      "batch 5500 : loss 0.000043\n",
      "batch 5600 : loss 0.000213\n",
      "batch 5700 : loss 0.001126\n",
      "batch 5800 : loss 0.005147\n",
      "batch 5900 : loss 0.001317\n",
      "test accuracy: 1.000000\n",
      "test accuracy: 0.990000\n",
      "test accuracy: 0.993333\n",
      "test accuracy: 0.995000\n",
      "test accuracy: 0.992000\n",
      "test accuracy: 0.993333\n",
      "test accuracy: 0.991429\n",
      "test accuracy: 0.992500\n",
      "test accuracy: 0.988889\n",
      "test accuracy: 0.990000\n",
      "test accuracy: 0.990909\n",
      "test accuracy: 0.990000\n",
      "test accuracy: 0.989231\n",
      "test accuracy: 0.988571\n",
      "test accuracy: 0.988000\n",
      "test accuracy: 0.988750\n",
      "test accuracy: 0.989412\n",
      "test accuracy: 0.990000\n",
      "test accuracy: 0.988421\n",
      "test accuracy: 0.989000\n",
      "test accuracy: 0.987619\n",
      "test accuracy: 0.988182\n",
      "test accuracy: 0.986957\n",
      "test accuracy: 0.987500\n",
      "test accuracy: 0.987200\n",
      "test accuracy: 0.986923\n",
      "test accuracy: 0.986667\n",
      "test accuracy: 0.986429\n",
      "test accuracy: 0.986897\n",
      "test accuracy: 0.986667\n",
      "test accuracy: 0.986452\n",
      "test accuracy: 0.986875\n",
      "test accuracy: 0.987273\n",
      "test accuracy: 0.987647\n",
      "test accuracy: 0.986857\n",
      "test accuracy: 0.987222\n",
      "test accuracy: 0.987568\n",
      "test accuracy: 0.987368\n",
      "test accuracy: 0.987179\n",
      "test accuracy: 0.987500\n",
      "test accuracy: 0.987317\n",
      "test accuracy: 0.987619\n",
      "test accuracy: 0.986512\n",
      "test accuracy: 0.986818\n",
      "test accuracy: 0.987111\n",
      "test accuracy: 0.986957\n",
      "test accuracy: 0.987234\n",
      "test accuracy: 0.987500\n",
      "test accuracy: 0.987755\n",
      "test accuracy: 0.987600\n",
      "test accuracy: 0.987843\n",
      "test accuracy: 0.987692\n",
      "test accuracy: 0.987925\n",
      "test accuracy: 0.987778\n",
      "test accuracy: 0.988000\n",
      "test accuracy: 0.987857\n",
      "test accuracy: 0.988070\n",
      "test accuracy: 0.987931\n",
      "test accuracy: 0.987797\n",
      "test accuracy: 0.987667\n",
      "test accuracy: 0.987869\n",
      "test accuracy: 0.987419\n",
      "test accuracy: 0.987619\n",
      "test accuracy: 0.987813\n",
      "test accuracy: 0.988000\n",
      "test accuracy: 0.988182\n",
      "test accuracy: 0.988358\n",
      "test accuracy: 0.988529\n",
      "test accuracy: 0.988406\n",
      "test accuracy: 0.988286\n",
      "test accuracy: 0.987887\n",
      "test accuracy: 0.987778\n",
      "test accuracy: 0.987945\n",
      "test accuracy: 0.988108\n",
      "test accuracy: 0.988000\n",
      "test accuracy: 0.987895\n",
      "test accuracy: 0.988052\n",
      "test accuracy: 0.988205\n",
      "test accuracy: 0.988101\n",
      "test accuracy: 0.987750\n",
      "test accuracy: 0.987901\n",
      "test accuracy: 0.988049\n",
      "test accuracy: 0.988193\n",
      "test accuracy: 0.987857\n",
      "test accuracy: 0.988000\n",
      "test accuracy: 0.987674\n",
      "test accuracy: 0.987816\n",
      "test accuracy: 0.987500\n",
      "test accuracy: 0.987640\n",
      "test accuracy: 0.987778\n",
      "test accuracy: 0.987692\n",
      "test accuracy: 0.987609\n",
      "test accuracy: 0.987742\n",
      "test accuracy: 0.987660\n",
      "test accuracy: 0.987579\n",
      "test accuracy: 0.987500\n",
      "test accuracy: 0.987216\n",
      "test accuracy: 0.987347\n",
      "test accuracy: 0.987475\n",
      "test accuracy: 0.987600\n",
      "test accuracy: 0.987723\n",
      "test accuracy: 0.987843\n",
      "test accuracy: 0.987961\n",
      "test accuracy: 0.988077\n",
      "test accuracy: 0.988190\n",
      "test accuracy: 0.988302\n",
      "test accuracy: 0.988411\n",
      "test accuracy: 0.988519\n",
      "test accuracy: 0.988624\n",
      "test accuracy: 0.988727\n",
      "test accuracy: 0.988829\n",
      "test accuracy: 0.988929\n",
      "test accuracy: 0.989027\n",
      "test accuracy: 0.989123\n",
      "test accuracy: 0.989217\n",
      "test accuracy: 0.989310\n",
      "test accuracy: 0.989402\n",
      "test accuracy: 0.989492\n",
      "test accuracy: 0.989412\n",
      "test accuracy: 0.989333\n",
      "test accuracy: 0.989421\n",
      "test accuracy: 0.989344\n",
      "test accuracy: 0.989431\n",
      "test accuracy: 0.989516\n",
      "test accuracy: 0.989600\n",
      "test accuracy: 0.989683\n",
      "test accuracy: 0.989764\n",
      "test accuracy: 0.989844\n",
      "test accuracy: 0.989922\n",
      "test accuracy: 0.990000\n",
      "test accuracy: 0.990076\n",
      "test accuracy: 0.989848\n",
      "test accuracy: 0.989774\n",
      "test accuracy: 0.989851\n",
      "test accuracy: 0.989926\n",
      "test accuracy: 0.990000\n",
      "test accuracy: 0.990073\n",
      "test accuracy: 0.990145\n",
      "test accuracy: 0.990216\n",
      "test accuracy: 0.990286\n",
      "test accuracy: 0.990355\n",
      "test accuracy: 0.990423\n",
      "test accuracy: 0.990489\n",
      "test accuracy: 0.990556\n",
      "test accuracy: 0.990621\n",
      "test accuracy: 0.990685\n",
      "test accuracy: 0.990748\n",
      "test accuracy: 0.990811\n",
      "test accuracy: 0.990873\n",
      "test accuracy: 0.990933\n",
      "test accuracy: 0.990993\n",
      "test accuracy: 0.991053\n",
      "test accuracy: 0.991111\n",
      "test accuracy: 0.991169\n",
      "test accuracy: 0.991226\n",
      "test accuracy: 0.991282\n",
      "test accuracy: 0.991338\n",
      "test accuracy: 0.991392\n",
      "test accuracy: 0.991447\n",
      "test accuracy: 0.991500\n",
      "test accuracy: 0.991553\n",
      "test accuracy: 0.991481\n",
      "test accuracy: 0.991534\n",
      "test accuracy: 0.991585\n",
      "test accuracy: 0.991636\n",
      "test accuracy: 0.991687\n",
      "test accuracy: 0.991737\n",
      "test accuracy: 0.991786\n",
      "test accuracy: 0.991716\n",
      "test accuracy: 0.991765\n",
      "test accuracy: 0.991813\n",
      "test accuracy: 0.991860\n",
      "test accuracy: 0.991908\n",
      "test accuracy: 0.991954\n",
      "test accuracy: 0.992000\n",
      "test accuracy: 0.992045\n",
      "test accuracy: 0.992090\n",
      "test accuracy: 0.992135\n",
      "test accuracy: 0.992179\n",
      "test accuracy: 0.992222\n",
      "test accuracy: 0.992265\n",
      "test accuracy: 0.992308\n",
      "test accuracy: 0.992350\n",
      "test accuracy: 0.992391\n",
      "test accuracy: 0.992432\n",
      "test accuracy: 0.992473\n",
      "test accuracy: 0.992513\n",
      "test accuracy: 0.992553\n",
      "test accuracy: 0.992593\n",
      "test accuracy: 0.992632\n",
      "test accuracy: 0.992670\n",
      "test accuracy: 0.992708\n",
      "test accuracy: 0.992746\n",
      "test accuracy: 0.992577\n",
      "test accuracy: 0.992513\n",
      "test accuracy: 0.992551\n",
      "test accuracy: 0.992487\n",
      "test accuracy: 0.992525\n",
      "test accuracy: 0.992563\n",
      "test accuracy: 0.992600\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # MNIST中的图像默认为uint8（0-255的数字）。以下代码将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        # 从数据集中随机取出batch_size个元素并返回\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]\n",
    "\n",
    "#-------------------------------------------------------\n",
    "#模型的训练\n",
    "num_epochs = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "#此处切换模型\n",
    "model = CNN()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "\n",
    "#-------------------------------------------------------\n",
    "#从 DataLoader 中随机取一批训练数据；\n",
    "#将这批数据送入模型，计算出模型的预测值；\n",
    "#将模型预测值与真实值进行比较，计算损失函数（loss）。这里使用 tf.keras.losses 中的交叉熵函数作为损失函数；\n",
    "#计算损失函数关于模型变量的导数；\n",
    "#将求出的导数值传入优化器，使用优化器的 apply_gradients 方法更新模型参数以最小化损失函数（优化器的详细使用方法见 前章 ）。\n",
    "#-------------------------------------------------------\n",
    "\n",
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true = y, y_pred = y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        if batch_index % 100 == 0:\n",
    "            print(\"batch {:d} : loss {:f}\".format(batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars = zip(grads, model.variables))\n",
    "#-------------------------------------------------------\n",
    "\n",
    "#-------------------------------------------------------\n",
    "#模型的评估\n",
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index : end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true = data_loader.test_label[start_index : end_index], y_pred = y_pred)\n",
    "    print(\"test accuracy: {:f}\".format(sparse_categorical_accuracy.result()))\n",
    "\n",
    "\n",
    "#-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <加载预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mDownloading and preparing dataset tf_flowers/3.0.1 (download: 218.21 MiB, generated: 221.83 MiB, total: 440.05 MiB) to /Users/zsf/tensorflow_datasets/tf_flowers/3.0.1...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Dataset tf_flowers is hosted on GCS. It will automatically be downloaded to your\n",
      "local data directory. If you'd instead prefer to read directly from our public\n",
      "GCS bucket (recommended if you're running on GCP), you can instead pass\n",
      "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f5cd7f478643fd8e36c20998e2412e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Dl Completed...', max=5.0, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1mDataset tf_flowers downloaded and prepared to /Users/zsf/tensorflow_datasets/tf_flowers/3.0.1. Subsequent calls will reuse this data.\u001B[0m\n",
      "loss 1.659369\n",
      "loss 1.626653\n",
      "loss 1.854700\n",
      "loss 1.719930\n",
      "loss 1.521780\n",
      "loss 1.702531\n",
      "loss 1.721047\n",
      "loss 1.711823\n",
      "loss 1.651302\n",
      "loss 1.616478\n",
      "loss 1.547647\n",
      "loss 1.444698\n",
      "loss 1.739620\n",
      "loss 1.652479\n",
      "loss 1.615724\n",
      "loss 1.598410\n",
      "loss 1.423571\n",
      "loss 1.544024\n",
      "loss 1.305873\n",
      "loss 1.612223\n",
      "loss 1.638282\n",
      "loss 1.650394\n",
      "loss 1.553843\n",
      "loss 1.487343\n",
      "loss 1.695044\n",
      "loss 1.420199\n",
      "loss 1.326383\n",
      "loss 1.581424\n",
      "loss 1.572337\n",
      "loss 1.338717\n",
      "loss 1.322334\n",
      "loss 1.496048\n",
      "loss 1.414958\n",
      "loss 1.198110\n",
      "loss 1.330192\n",
      "loss 1.333858\n",
      "loss 1.285416\n",
      "loss 1.367896\n",
      "loss 1.313110\n",
      "loss 1.317251\n",
      "loss 1.597445\n",
      "loss 1.257945\n",
      "loss 1.440295\n",
      "loss 1.181853\n",
      "loss 1.171274\n",
      "loss 1.756959\n",
      "loss 1.289944\n",
      "loss 1.004329\n",
      "loss 1.603307\n",
      "loss 1.340686\n",
      "loss 1.294433\n",
      "loss 1.349244\n",
      "loss 1.553134\n",
      "loss 1.261371\n",
      "loss 1.209425\n",
      "loss 1.261326\n",
      "loss 1.169861\n",
      "loss 1.446637\n",
      "loss 1.171175\n",
      "loss 1.234749\n",
      "loss 1.148795\n",
      "loss 1.243402\n",
      "loss 1.120327\n",
      "loss 1.287183\n",
      "loss 1.369857\n",
      "loss 1.537411\n",
      "loss 1.115000\n",
      "loss 1.462047\n",
      "loss 1.026804\n",
      "loss 1.242183\n",
      "loss 0.928262\n",
      "loss 1.142011\n",
      "loss 1.209943\n",
      "loss 1.144992\n",
      "tf.Tensor(\n",
      "[[4.3920982e-01 2.0153365e-01 2.5665966e-01 3.8860440e-02 6.3736387e-02]\n",
      " [3.5420665e-01 4.2759955e-01 8.4223397e-02 8.5659511e-02 4.8310846e-02]\n",
      " [3.2846311e-01 9.3114540e-02 4.0569004e-01 8.5417628e-02 8.7314658e-02]\n",
      " [4.2719956e-02 2.8855851e-05 3.7349761e-04 9.5506942e-01 1.8082536e-03]\n",
      " [6.0766786e-02 9.8206647e-02 6.1373770e-01 3.3176716e-02 1.9411206e-01]\n",
      " [3.0431163e-01 1.6821578e-01 7.8743607e-02 4.0274739e-01 4.5981467e-02]\n",
      " [2.3653266e-01 7.9660103e-02 4.6326143e-01 1.4443320e-01 7.6112635e-02]\n",
      " [2.2870377e-01 1.8603353e-01 8.2484625e-02 4.1436282e-01 8.8415310e-02]\n",
      " [1.6400654e-02 2.6677041e-03 8.7999791e-01 1.5001803e-02 8.5931912e-02]\n",
      " [1.2693252e-01 3.7941390e-01 2.9615337e-01 1.2320846e-01 7.4291758e-02]\n",
      " [4.8075697e-01 2.7409112e-01 1.7583282e-01 3.2936767e-02 3.6382299e-02]\n",
      " [5.2400004e-02 9.2017241e-03 5.9427857e-01 3.5632279e-02 3.0848742e-01]\n",
      " [6.2458092e-01 1.4874960e-01 1.5684868e-01 3.2728117e-02 3.7092641e-02]\n",
      " [7.7163112e-01 9.7137965e-02 8.6546935e-02 2.1703931e-02 2.2980010e-02]\n",
      " [1.5540393e-01 6.7681330e-01 3.0571532e-02 8.2213022e-02 5.4998271e-02]\n",
      " [4.6162035e-02 6.9791190e-02 6.8942356e-01 4.3380551e-02 1.5124267e-01]\n",
      " [2.3316487e-03 4.0919171e-04 5.5124032e-01 2.9310681e-02 4.1670820e-01]\n",
      " [1.6880384e-01 4.9341280e-02 5.7001102e-01 2.8203158e-02 1.8364069e-01]\n",
      " [2.6715007e-01 2.5439161e-01 3.3370376e-01 5.8177587e-02 8.6576916e-02]\n",
      " [1.3774173e-01 1.7715554e-01 5.9089822e-01 1.7625792e-02 7.6578766e-02]], shape=(20, 5), dtype=float32)\n",
      "loss 1.209418\n",
      "loss 0.997165\n",
      "loss 1.185089\n",
      "loss 1.147329\n",
      "loss 1.200193\n",
      "loss 0.988530\n",
      "loss 1.002297\n",
      "loss 1.073402\n",
      "loss 1.138540\n",
      "loss 1.069366\n",
      "loss 1.219954\n",
      "loss 1.066690\n",
      "loss 1.494021\n",
      "loss 1.141189\n",
      "loss 1.064306\n",
      "loss 0.976828\n",
      "loss 0.824543\n",
      "loss 0.958927\n",
      "loss 1.460604\n",
      "loss 1.314614\n",
      "loss 1.441445\n",
      "loss 0.948714\n",
      "loss 1.320653\n",
      "loss 0.926388\n",
      "loss 1.112615\n",
      "loss 1.404518\n",
      "loss 1.091081\n",
      "loss 0.991289\n",
      "loss 1.205245\n",
      "loss 0.940487\n",
      "loss 0.990668\n",
      "loss 0.990475\n",
      "loss 1.001467\n",
      "loss 0.985401\n",
      "loss 0.920175\n",
      "loss 0.989830\n",
      "loss 1.246715\n",
      "loss 1.022896\n",
      "loss 0.923892\n",
      "loss 1.101577\n",
      "loss 1.188226\n",
      "loss 1.214490\n",
      "loss 1.259031\n",
      "loss 1.183363\n",
      "loss 0.861694\n",
      "loss 1.070200\n",
      "loss 0.913121\n",
      "loss 0.941824\n",
      "loss 0.995777\n",
      "loss 1.026538\n",
      "loss 0.959540\n",
      "loss 0.876405\n",
      "loss 0.879409\n",
      "loss 0.913554\n",
      "loss 0.778246\n",
      "loss 1.091724\n",
      "loss 0.903972\n",
      "loss 0.856836\n",
      "loss 0.762334\n",
      "loss 1.158289\n",
      "loss 0.985558\n",
      "loss 1.075831\n",
      "loss 0.955152\n",
      "loss 1.033633\n",
      "loss 1.021779\n",
      "loss 0.912212\n",
      "loss 1.230581\n",
      "loss 1.048590\n",
      "loss 0.902742\n",
      "loss 0.749658\n",
      "loss 0.802415\n",
      "loss 1.276663\n",
      "loss 0.956367\n",
      "loss 1.229925\n",
      "tf.Tensor(\n",
      "[[9.3737990e-01 3.2493576e-02 1.3183357e-02 5.4697692e-03 1.1473337e-02]\n",
      " [1.5975269e-02 1.7817815e-01 1.5229091e-01 6.3300902e-01 2.0546678e-02]\n",
      " [8.5620111e-01 4.2871378e-02 4.4759247e-02 1.6389024e-02 3.9779216e-02]\n",
      " [6.8837658e-02 1.6573545e-03 5.6429427e-02 8.6078835e-01 1.2287177e-02]\n",
      " [2.4776959e-03 9.6067232e-01 8.7590078e-03 1.4754659e-02 1.3336284e-02]\n",
      " [6.0193767e-03 2.4912549e-02 4.0132517e-01 1.6356721e-03 5.6610727e-01]\n",
      " [7.4025875e-01 7.0532188e-02 7.4772693e-02 6.7946658e-02 4.6489660e-02]\n",
      " [5.0700170e-01 3.5390610e-01 6.3697778e-02 4.0896557e-02 3.4497868e-02]\n",
      " [1.1068487e-01 2.5764799e-01 4.4864851e-01 9.4259784e-02 8.8758878e-02]\n",
      " [6.0198584e-04 8.7712169e-06 2.9762744e-03 9.9615592e-01 2.5708094e-04]\n",
      " [5.9376186e-01 1.7801481e-01 1.0014712e-01 2.1320289e-02 1.0675584e-01]\n",
      " [1.3410728e-04 1.2383708e-03 4.3725345e-01 4.5125937e-05 5.6132889e-01]\n",
      " [6.7145652e-01 5.5260655e-02 1.4151080e-01 5.3439092e-02 7.8332879e-02]\n",
      " [4.1074168e-02 1.0409346e-02 2.2912795e-02 9.1685367e-01 8.7500717e-03]\n",
      " [7.2172873e-02 5.1846981e-01 8.9889817e-02 1.9796234e-01 1.2150504e-01]\n",
      " [3.9993919e-02 5.5261868e-01 2.3835021e-01 1.9242197e-02 1.4979495e-01]\n",
      " [7.4977291e-01 5.4194208e-02 6.5048419e-02 9.5074825e-02 3.5909489e-02]\n",
      " [8.4768951e-01 5.2528255e-02 5.9174042e-02 1.0748398e-02 2.9859740e-02]\n",
      " [2.4453190e-01 3.8280368e-01 1.8210799e-01 1.1169940e-02 1.7938644e-01]\n",
      " [3.8239345e-01 8.6283945e-02 1.9917457e-01 9.9165358e-02 2.3298267e-01]], shape=(20, 5), dtype=float32)\n",
      "loss 0.953645\n",
      "loss 1.123049\n",
      "loss 1.153217\n",
      "loss 1.069445\n",
      "loss 1.230046\n",
      "loss 1.235766\n",
      "loss 1.097283\n",
      "loss 0.890817\n",
      "loss 0.906726\n",
      "loss 0.943676\n",
      "loss 1.057281\n",
      "loss 1.116723\n",
      "loss 0.733550\n",
      "loss 1.149458\n",
      "loss 1.193512\n",
      "loss 0.824023\n",
      "loss 0.765816\n",
      "loss 1.079515\n",
      "loss 0.824674\n",
      "loss 0.753847\n",
      "loss 0.961006\n",
      "loss 0.968433\n",
      "loss 0.785838\n",
      "loss 0.866766\n",
      "loss 0.813441\n",
      "loss 0.989511\n",
      "loss 0.827480\n",
      "loss 0.707610\n",
      "loss 0.835372\n",
      "loss 1.136443\n",
      "loss 0.777358\n",
      "loss 0.965760\n",
      "loss 0.776915\n",
      "loss 1.005449\n",
      "loss 0.768827\n",
      "loss 1.025405\n",
      "loss 0.882091\n",
      "loss 0.864797\n",
      "loss 0.719879\n",
      "loss 0.898094\n",
      "loss 0.798061\n",
      "loss 0.778072\n",
      "loss 0.827931\n",
      "loss 0.777004\n",
      "loss 0.762118\n",
      "loss 1.057792\n",
      "loss 0.746183\n",
      "loss 0.870057\n",
      "loss 0.655880\n",
      "loss 0.817001\n",
      "loss 1.000461\n",
      "loss 0.771535\n",
      "loss 0.688146\n",
      "loss 0.884961\n",
      "loss 1.030470\n",
      "loss 0.795579\n",
      "loss 0.769884\n",
      "loss 0.753264\n",
      "loss 0.828255\n",
      "loss 0.465310\n",
      "loss 1.075934\n",
      "loss 0.954110\n",
      "loss 0.641127\n",
      "loss 0.725299\n",
      "loss 0.872897\n",
      "loss 0.540279\n",
      "loss 0.638859\n",
      "loss 0.792098\n",
      "loss 0.705299\n",
      "loss 0.792991\n",
      "loss 0.990863\n",
      "loss 0.585718\n",
      "loss 0.734567\n",
      "loss 1.191649\n",
      "tf.Tensor(\n",
      "[[9.0702206e-02 8.2579777e-03 2.7618185e-01 6.0800886e-01 1.6849052e-02]\n",
      " [9.6145517e-01 3.0007560e-02 3.9087050e-03 1.9235173e-03 2.7049927e-03]\n",
      " [8.5572928e-02 2.9235876e-01 3.2309017e-01 6.8879075e-02 2.3009910e-01]\n",
      " [1.4055137e-03 2.1157128e-01 7.1410835e-01 5.4621208e-03 6.7452744e-02]\n",
      " [1.5113279e-02 3.4819030e-07 1.8869595e-03 9.8272175e-01 2.7762022e-04]\n",
      " [1.2114498e-05 9.3252587e-05 9.9972945e-01 3.5735866e-05 1.2946702e-04]\n",
      " [6.6482313e-02 1.7666839e-03 4.9228407e-02 8.2979691e-01 5.2725703e-02]\n",
      " [2.6121330e-01 1.5861900e-01 1.7096756e-01 1.7152947e-01 2.3767072e-01]\n",
      " [4.9144042e-01 3.8847870e-01 4.8907146e-02 2.4508001e-02 4.6665747e-02]\n",
      " [1.5611362e-01 3.2239500e-01 3.3344799e-01 4.8277017e-02 1.3976629e-01]\n",
      " [6.1611998e-01 6.0838494e-03 3.5833273e-02 2.8798509e-01 5.3977754e-02]\n",
      " [8.0654287e-01 1.5131541e-01 1.9373775e-02 3.3807307e-03 1.9387124e-02]\n",
      " [1.8228510e-02 9.4413275e-01 5.8142203e-03 2.0428400e-02 1.1396231e-02]\n",
      " [2.7488543e-06 3.0282981e-04 4.9059325e-01 7.9037774e-05 5.0902218e-01]\n",
      " [8.1513403e-03 5.9463668e-01 3.0653048e-01 5.4918644e-03 8.5189722e-02]\n",
      " [1.2023696e-01 1.1989159e-01 1.9146629e-01 5.3854239e-01 2.9862763e-02]\n",
      " [9.4328374e-01 2.6981551e-02 7.0268265e-03 1.2929261e-02 9.7786551e-03]\n",
      " [9.5773429e-01 3.5691068e-02 3.7821708e-03 6.5605697e-04 2.1364274e-03]\n",
      " [9.1238514e-02 1.1375050e-01 6.7080563e-01 2.4980096e-02 9.9225216e-02]\n",
      " [9.9819235e-04 9.1434242e-03 9.6108639e-01 1.0747778e-03 2.7697176e-02]], shape=(20, 5), dtype=float32)\n",
      "loss 0.694298\n",
      "loss 0.653994\n",
      "loss 0.934159\n",
      "loss 0.747361\n",
      "loss 0.600938\n",
      "loss 1.196037\n",
      "loss 0.846289\n",
      "loss 1.038360\n",
      "loss 0.639025\n",
      "loss 0.920664\n",
      "loss 0.947728\n",
      "loss 0.951886\n",
      "loss 0.630378\n",
      "loss 0.789408\n",
      "loss 1.042815\n",
      "loss 0.570081\n",
      "loss 0.931143\n",
      "loss 0.683362\n",
      "loss 0.579277\n",
      "loss 0.885977\n",
      "loss 0.886563\n",
      "loss 0.832631\n",
      "loss 0.560438\n",
      "loss 0.698930\n",
      "loss 0.951364\n",
      "loss 1.002953\n",
      "loss 1.094640\n",
      "loss 0.782284\n",
      "loss 0.809057\n",
      "loss 0.590856\n",
      "loss 0.928917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.839323\n",
      "loss 0.883764\n",
      "loss 0.629678\n",
      "loss 0.695934\n",
      "loss 0.756465\n",
      "loss 0.734148\n",
      "loss 0.640935\n",
      "loss 0.644855\n",
      "loss 0.870543\n",
      "loss 0.828684\n",
      "loss 0.627383\n",
      "loss 1.158207\n",
      "loss 0.522141\n",
      "loss 0.826383\n",
      "loss 0.803859\n",
      "loss 0.582995\n",
      "loss 0.646660\n",
      "loss 0.589244\n",
      "loss 0.669635\n",
      "loss 0.701248\n",
      "loss 0.819633\n",
      "loss 0.590893\n",
      "loss 0.675940\n",
      "loss 0.627448\n",
      "loss 0.883757\n",
      "loss 0.612456\n",
      "loss 0.893399\n",
      "loss 0.694932\n",
      "loss 0.545489\n",
      "loss 0.482694\n",
      "loss 0.612580\n",
      "loss 0.515126\n",
      "loss 0.521856\n",
      "loss 0.951189\n",
      "loss 0.511341\n",
      "loss 0.811320\n",
      "loss 0.512512\n",
      "loss 0.889187\n",
      "loss 0.559674\n",
      "loss 0.949538\n",
      "loss 0.709220\n",
      "loss 0.650799\n",
      "loss 0.899638\n",
      "tf.Tensor(\n",
      "[[1.74888343e-01 8.65133479e-02 2.63797790e-01 2.46691797e-02\n",
      "  4.50131238e-01]\n",
      " [4.44245670e-04 1.26690924e-04 2.62346923e-01 3.92854010e-04\n",
      "  7.36689210e-01]\n",
      " [1.93372682e-01 8.79162364e-03 5.77388844e-03 7.90941358e-01\n",
      "  1.12039002e-03]\n",
      " [4.58042523e-05 1.60617928e-05 9.86060977e-01 2.89278716e-04\n",
      "  1.35879675e-02]\n",
      " [3.23473290e-02 9.17320013e-01 2.06714645e-02 2.22334843e-02\n",
      "  7.42777623e-03]\n",
      " [4.18197270e-03 9.93517876e-01 6.80364028e-04 7.36160553e-04\n",
      "  8.83504690e-04]\n",
      " [8.27786505e-01 3.66608147e-03 2.33049202e-03 1.65029928e-01\n",
      "  1.18705223e-03]\n",
      " [9.87747550e-01 4.08869935e-03 1.90742791e-03 5.45898685e-03\n",
      "  7.97428773e-04]\n",
      " [1.19792067e-01 2.58638375e-02 1.30863219e-01 5.79070784e-02\n",
      "  6.65573776e-01]\n",
      " [4.16581461e-04 1.22594269e-04 9.87260342e-01 3.24765057e-03\n",
      "  8.95289611e-03]\n",
      " [2.71614164e-01 7.60186417e-03 2.29103640e-01 4.49897170e-01\n",
      "  4.17831689e-02]\n",
      " [9.55716670e-01 1.95487067e-02 4.78266133e-03 1.64409634e-03\n",
      "  1.83077604e-02]\n",
      " [3.96051968e-04 4.03992075e-04 7.22150147e-01 1.67221692e-03\n",
      "  2.75377601e-01]\n",
      " [1.00744888e-01 3.66578400e-01 3.75351191e-01 1.06157400e-01\n",
      "  5.11680841e-02]\n",
      " [1.44379940e-02 5.23545081e-03 3.75279874e-01 1.97002470e-01\n",
      "  4.08044249e-01]\n",
      " [1.20370612e-01 3.20336670e-01 1.79168284e-01 3.43574017e-01\n",
      "  3.65504697e-02]\n",
      " [2.64494330e-01 1.76281165e-02 1.86861202e-01 1.08923122e-01\n",
      "  4.22093302e-01]\n",
      " [4.64042574e-02 1.58627301e-01 2.16085330e-01 5.08086741e-01\n",
      "  7.07963258e-02]\n",
      " [2.07921639e-05 9.99942660e-01 1.72930595e-05 1.28042566e-05\n",
      "  6.51603659e-06]\n",
      " [8.42325628e-01 9.91783962e-02 2.06467230e-02 3.19274259e-03\n",
      "  3.46565470e-02]], shape=(20, 5), dtype=float32)\n",
      "loss 0.566790\n",
      "loss 0.557212\n",
      "loss 0.635236\n",
      "loss 0.761159\n",
      "loss 0.886830\n",
      "loss 0.827969\n",
      "loss 0.646728\n",
      "loss 0.563656\n",
      "loss 1.013814\n",
      "loss 0.768606\n",
      "loss 0.735407\n",
      "loss 0.513092\n",
      "loss 0.910586\n",
      "loss 0.618909\n",
      "loss 0.588554\n",
      "loss 0.855626\n",
      "loss 0.613465\n",
      "loss 0.654236\n",
      "loss 0.728213\n",
      "loss 0.753892\n",
      "loss 0.522638\n",
      "loss 0.958552\n",
      "loss 0.734397\n",
      "loss 0.617052\n",
      "loss 0.600636\n",
      "loss 0.762826\n",
      "loss 0.533471\n",
      "loss 0.483088\n",
      "loss 0.497867\n",
      "loss 0.806087\n",
      "loss 0.672256\n",
      "loss 0.793903\n",
      "loss 0.663600\n",
      "loss 0.572019\n",
      "loss 0.632705\n",
      "loss 0.587092\n",
      "loss 0.710870\n",
      "loss 0.857811\n",
      "loss 0.700492\n",
      "loss 0.664491\n",
      "loss 0.573010\n",
      "loss 0.703727\n",
      "loss 0.670811\n",
      "loss 0.669772\n",
      "loss 0.767201\n",
      "loss 0.600737\n",
      "loss 0.616906\n",
      "loss 0.446672\n",
      "loss 0.663736\n",
      "loss 0.751099\n",
      "loss 0.676711\n",
      "loss 0.945431\n",
      "loss 0.741982\n",
      "loss 0.632197\n",
      "loss 0.861599\n",
      "loss 0.475366\n",
      "loss 0.776327\n",
      "loss 0.581407\n",
      "loss 0.696823\n",
      "loss 0.455315\n",
      "loss 0.805512\n",
      "loss 0.744082\n",
      "loss 0.873234\n",
      "loss 0.836770\n",
      "loss 0.733718\n",
      "loss 0.627524\n",
      "loss 0.606592\n",
      "loss 0.717137\n",
      "loss 0.696154\n",
      "loss 0.798342\n",
      "loss 0.898462\n",
      "loss 0.673407\n",
      "loss 0.846489\n",
      "loss 0.640006\n",
      "tf.Tensor(\n",
      "[[1.79010201e-02 9.30268288e-01 2.36629439e-03 3.94073501e-02\n",
      "  1.00570060e-02]\n",
      " [3.96409817e-02 1.20521411e-02 5.69391787e-01 3.75114024e-01\n",
      "  3.80107085e-03]\n",
      " [9.42249596e-01 1.75263416e-02 6.75210403e-03 3.67017160e-03\n",
      "  2.98017412e-02]\n",
      " [1.38668925e-01 8.38348567e-01 1.63399184e-03 5.76909073e-03\n",
      "  1.55794388e-02]\n",
      " [9.61789966e-01 1.06934924e-02 2.47162650e-03 3.39391665e-03\n",
      "  2.16510911e-02]\n",
      " [9.46168602e-01 8.78409762e-03 3.51396599e-03 3.96080278e-02\n",
      "  1.92524830e-03]\n",
      " [3.21237743e-02 2.31888015e-02 2.84116030e-01 7.06508011e-02\n",
      "  5.89920640e-01]\n",
      " [7.25158632e-01 2.22188905e-01 1.46174794e-02 1.01774568e-02\n",
      "  2.78576221e-02]\n",
      " [7.74423182e-02 2.29759831e-02 2.86768600e-02 8.67797852e-01\n",
      "  3.10704694e-03]\n",
      " [5.52250305e-04 1.92052987e-03 4.94944304e-02 4.53141145e-03\n",
      "  9.43501413e-01]\n",
      " [6.69523627e-02 6.46875342e-05 2.34671240e-03 9.29338992e-01\n",
      "  1.29719812e-03]\n",
      " [1.99157628e-03 1.14411935e-02 5.32020807e-01 3.67350830e-03\n",
      "  4.50872928e-01]\n",
      " [4.08552913e-03 1.84787605e-02 9.64661419e-01 8.28194723e-04\n",
      "  1.19461063e-02]\n",
      " [5.36495857e-02 7.20251501e-02 5.76257110e-01 6.90485612e-02\n",
      "  2.29019582e-01]\n",
      " [1.09328120e-03 3.00909649e-03 9.89392400e-01 4.87705634e-04\n",
      "  6.01754151e-03]\n",
      " [1.32687777e-01 7.18311191e-01 1.28365327e-02 6.69898465e-02\n",
      "  6.91747367e-02]\n",
      " [1.24309387e-03 8.00529774e-03 1.96763873e-01 5.93925593e-04\n",
      "  7.93393791e-01]\n",
      " [1.42315224e-01 2.43308698e-03 5.61992638e-02 7.93041348e-01\n",
      "  6.01109490e-03]\n",
      " [1.19380370e-01 1.98155365e-04 4.37798956e-03 8.75966311e-01\n",
      "  7.72122366e-05]\n",
      " [6.83514595e-01 1.41628817e-01 2.48936508e-02 3.58703942e-03\n",
      "  1.46375865e-01]], shape=(20, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "num_epoch = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "dataset = tfds.load(\"tf_flowers\", split = tfds.Split.TRAIN, as_supervised = True)\n",
    "dataset = dataset.map(lambda img, label : \n",
    "                      (tf.image.resize(img, (224, 224)) / 255.0, label)).shuffle(1024).batch(batch_size)\n",
    "model = tf.keras.applications.MobileNetV2(weights = None, classes = 5)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "for e in range(num_epoch):\n",
    "    for images, labels in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            labels_pred = model(images, training = True)\n",
    "            loss = tf.keras.losses.sparse_categorical_crossentropy(y_true = labels, y_pred = labels_pred)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            print(\"loss {:f}\".format(loss.numpy()))\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(grads_and_vars = zip(grads, model.trainable_variables))\n",
    "    print(labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}