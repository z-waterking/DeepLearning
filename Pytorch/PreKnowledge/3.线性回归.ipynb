{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归\n",
    "\n",
    "给定一个数据点集合$X$和对应的目标值$y$，线性模型的目标就是找到一条使用向量$w$和位移$b$描述的线，来尽可能地近似每个样本$X[i]$和$y[i]$。用数学符号来表示就是：\n",
    "$$\\hat{y} = Xw + b$$\n",
    "并最小化所有数据点上的平方误差\n",
    "$$\\sum_{i=1}^n (\\hat{y}_i-y_i)^2.$$\n",
    "\n",
    "接下来，我们会先手动写一个线性回归模型，然后再通过 nn 包构建一个线性回归模型。然后你自然而然地就会喜欢上用PyTorch了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的包\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn # 模型包，里面包含了各种各样的模型，方便我们直接使用\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成用来进行线性回归的模拟数据\n",
    "x = torch.unsqueeze(torch.linspace(-1, 1, 200), dim = 1)\n",
    "y = 5 * x + 0.8 * torch.rand(x.size())\n",
    "\n",
    "# 绘制模拟数据的图像\n",
    "plt.scatter(x.numpy(), y.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了能够自动求导，我们要将 x, y 变成 Variable 对象\n",
    "X = Variable(x) # PyTorch中的 Variable 默认是允许自动求导的，所以 requires_grad=True 可以不加\n",
    "Y = Variable(y) # 同上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义参数初始化函数\n",
    "def init_parameters():\n",
    "    W = Variable( torch.randn(1, 1), requires_grad=True)  # 随机初始化 w\n",
    "    b = Variable( torch.zeros(1, 1), requires_grad=True )  # 初始化偏差\n",
    "    parameters = {\"W\": W, \"b\": b}\n",
    "    return parameters\n",
    "\n",
    "# 定义模型\n",
    "def model(X, parameters):\n",
    "    return X * parameters[\"W\"] + parameters[\"b\"]\n",
    "\n",
    "# 定义损失函数\n",
    "def square_loss(y_hat, Y):\n",
    "    loss = (y_hat - Y).pow(2).sum()\n",
    "    return loss\n",
    "\n",
    "# 使用梯度来更新参数\n",
    "def update_parameters(parameters, lr):\n",
    "    parameters[\"W\"].data -= lr * parameters[\"W\"].grad.data\n",
    "    parameters[\"b\"].data -= lr * parameters[\"b\"].grad.data\n",
    "    return\n",
    "\n",
    "####     超参数     ####\n",
    "EPOCH = 100 # 迭代次数\n",
    "learning_rate = 0.001 # 学习速率\n",
    "\n",
    "parameters = init_parameters() # 参数初始化\n",
    "\n",
    "####     开始训练     ####\n",
    "for t in range(EPOCH):\n",
    "    # 对x进行预测\n",
    "    y_hat = model(X, parameters)\n",
    "    # 计算损失\n",
    "    loss = square_loss(y_hat, Y)\n",
    "    # 反向求导\n",
    "    loss.backward()\n",
    "    # 通过梯度，更新参数\n",
    "    update_parameters(parameters, learning_rate)\n",
    "    if (t+1) % 20 == 0:\n",
    "        print(loss)\n",
    "    # 因为自动求导会对梯度自动地积累，所以，我们要清除梯度\n",
    "    parameters[\"W\"].grad.data.zero_()\n",
    "    parameters[\"b\"].grad.data.zero_()\n",
    "\n",
    "# 画图\n",
    "plt.scatter(X.data.numpy(), Y.data.numpy())\n",
    "plt.plot(X.data.numpy(), y_hat.data.numpy(), 'r-', lw = 4)\n",
    "plt.show()\n",
    "\n",
    "print(\"实际的参数w是： 5 \\n\" )\n",
    "print(\"预测的参数w是\", parameters[\"W\"])\n",
    "print(\"预测的常数项是：\" , parameters[\"b\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用torch.nn来构建模型\n",
    "\n",
    "在PyTorch中 nn 包定义了一系列基本组件，这些组件（Modules）涵盖了大部分构建神经网络会使用的各种层。一个组件（Modules）的输入是Variable，经过组件之后，输出又是另一个 Variable。\n",
    "\n",
    "**当然nn包中还包含了大部分我们平时使用的损失函数**。\n",
    "\n",
    "**最后torch.optim中还含有很多我们平时使用的优化算法，用来更新梯度**。（我们上面使用的就是梯度下降法，只不过上面是我们自己手写，现在可以直接调用了）\n",
    "\n",
    "刚才，我们使用的是Tensor和autograd来构建线性回归的模型，现在，我们来使用torch.nn来快速构建一个线性回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 还是使用上述的X和Y\n",
    "\n",
    "X = Variable(x) # PyTorch中的 Variable 默认是允许自动求导的，所以 requires_grad=True 可以不加\n",
    "Y = Variable(y) # 同上\n",
    "\n",
    "####     超参数     ####\n",
    "EPOCH = 100 # 迭代次数\n",
    "learning_rate = 0.001 # 学习速率\n",
    "\n",
    "\n",
    "# 定义模型\n",
    "# 使用 nn包来定义我们的模型，这里的Linear表示的是线性模型，我们在初始化这个模型的时候，\n",
    "# 需要传入的参数是: in_features, out_features 也就是输出特征和输出特征\n",
    "# 默认会帮我们初始化权重，当然，我们也可以手动初始化权重（这里暂时不说）\n",
    "model = nn.Linear(1, 1)\n",
    "\n",
    "# 定义损失函数\n",
    "# 我们使用 Mean Square Loss作为我们的损失函数\n",
    "# size_average=False表示我们需要的是总的误差，不需要去平均误差\n",
    "square_loss = nn.MSELoss(size_average=False)\n",
    "\n",
    "\n",
    "# 定义优化方法\n",
    "# 以前，我们的梯度下降法都是我们手写的，现在，我们可以使用nn为我们封装好的。\n",
    "# 使用optim包来定义优化算法，可以自动的帮我们对模型的参数进行梯度更新\n",
    "# model.parameters()会自动的为我们将模型中的参数提取出来。然后我们告诉优化器，它们是我们要更新的参数。\n",
    "# lr表示的是学习速率\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "####     开始训练     ####\n",
    "for t in range(EPOCH):\n",
    "    \n",
    "    # 没有变化，还是这样使用：对x进行预测\n",
    "    y_hat = model(X)\n",
    "    \n",
    "    # 没有变化，计算损失\n",
    "    loss = square_loss(y_hat, Y)\n",
    "    \n",
    "    # 打印损失\n",
    "    if (t+1) % 20 == 0:\n",
    "        print(loss)\n",
    "    # 在我们反向求导之前，我们需要清空积累的梯度，由于我们使用的是 torch.optim包中的对象，我们可以直接调用\n",
    "    # 该对象的方法，来自动的清空积累的梯度\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 反向求导，也没变\n",
    "    loss.backward()\n",
    "    \n",
    "    # 反向求导结束，我们开始更新梯度，以前更新梯度需要手动输入w1.grad.data，现在只需要一行代码就可以搞定了！\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "\n",
    "# 画图\n",
    "plt.scatter(X.data.numpy(), Y.data.numpy())\n",
    "plt.plot(X.data.numpy(), y_hat.data.numpy(), 'r-', lw = 4)\n",
    "plt.show()\n",
    "\n",
    "print(\"实际的参数w是： 5 \\n\" )\n",
    "print(\"预测的参数w是\", parameters[\"W\"])\n",
    "print(\"预测的常数项是：\" , parameters[\"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
